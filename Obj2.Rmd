---
title: "Objective 2"
author: "Ahmed Awadallah"
date: '2022-07-30'
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
#Setup
library(aplore3)
library(car)
library(glmnet)
library(epitools)
library(randomForest)
library(MASS)
library(mvtnorm)

#Change working directory to this source file directory
setwd(dirname(rstudioapi::getSourceEditorContext()$path))

#Load my custom functions
source("Utility.r")
```

## Data Pre Process


```{r cars}
glow_bonemed_raw = glow_bonemed

#Don't need to do much cleaning we know this data is overall fine
#Remove features that should not be used in the model
#Choose variables
variablesToRemove = c("sub_id", "site_id")
glow_bonemed_raw = glow_bonemed_raw %>% select(-variablesToRemove)

#Split into training and test set
set.seed(1234)
train_test_list = get_train_test_list(glow_bonemed_raw , 0.8)
raw_train = train_test_list[[1]]
raw_test = train_test_list[[2]]

#Through our EDA we know the data is mostly clean we just need to handle the
#data imbalance

raw_train = get_df_with_upsampled_class(raw_train, "fracture", "Yes", 3)
```

## Setup Metric Comparison DF


```{r pressure, echo=FALSE}
#Start Model Building Process
model_compare_df = data.frame(model_num = c(0), model_description = c("Null Model"), auc=c(0), accuracy=c(0), balanced_accuracy=c(0)
                              , sensitivity=c(0), specificity=c(0), f1_score=c(0))
model_compare_df
model_num = 0

pos_class = "Yes"
```

## Complex Logistic Regression

```{r}
model_description = "Complex LogReg"

#Var Selection
variablesToSelect = c("phy_id", "priorfrac", "age", "height", "bmi", "premeno", "momfrac", 
                      "armassist", "raterisk", "bonemed", "bonemed_fu", 
                      "bonetreat", "fracture")
train = raw_train %>% select(variablesToSelect)
test = raw_test %>% select(variablesToSelect)

#Add New Cts Features
train$harmonic_age = 1/train$age
train$log_age =log(train$age)

train$harmonic_height = 1/train$height
train$log_height =log(train$height)

train$harmonic_bmi = 1/train$bmi
train$log_bmi =log(train$bmi)

test$harmonic_age = 1/test$age
test$log_age =log(test$age)

test$harmonic_height = 1/test$height
test$log_height =log(test$height)

test$harmonic_bmi = 1/test$bmi
test$log_bmi =log(test$bmi)

#Standardize Data
cts_vars = c("age", "harmonic_age", "log_age", "height", "harmonic_height", "log_height", "bmi", "harmonic_bmi", "log_bmi")
train = get_standardized_df(train, cts_vars)
test = get_standardized_df(test, cts_vars)

#Model
model_num = model_num + 1
model=glm(fracture~
            age +
            height + tan(harmonic_height) + tan(harmonic_height):bmi +
            bmi  + bmi:bonemed +
            momfrac+
            armassist+
            raterisk+
            bonemed+
            bonemed_fu +
            bonetreat
          ,family="binomial",data=train)
summary(model)

#Roc and AUC
auc = plot_roc_and_get_auc(model, test, "fracture")

#Best Threshold using Balanced Accuracy
preds = unname(predict(model, test, type="response"))
best_threshold = get_and_plot_best_threshold(preds, test$fracture, pos_class)
class_preds = ifelse(preds > best_threshold,"Yes","No")
CM_rep = confusionMatrix(table(class_preds, test$fracture), positive = pos_class)
acc = CM_rep$overall[1]
balanced_acc = CM_rep$byClass[11]
sens = CM_rep$byClass[1]
spec = CM_rep$byClass[2]
precision = CM_rep$byClass[5]
recall = CM_rep$byClass[6]
f1 = get_f1(precision, recall)

#Update Model Comparison Dataframe
model_stats = c(model_num, model_description, auc, acc, balanced_acc, sens, spec, f1)
model_compare_df = rbind(model_compare_df, model_stats)
```

## PCA and QDA

```{r}
model_description = "QDA + PC Space"

#Var Selection
variablesToSelect = c("phy_id", "age", "height", "bmi", "fracscore", "fracture")
norm_train = raw_train %>% select(variablesToSelect)
norm_test = raw_test %>% select(variablesToSelect)

#Standardize Data
cts_vars = c("phy_id", "age", "height", "bmi", "fracscore")
norm_train = get_standardized_df(norm_train, cts_vars)
norm_test = get_standardized_df(norm_test, cts_vars)

#Setup PC Train Test Dataframes
predictors = c("phy_id", "age", "height", "bmi", "fracscore")
pc_object = prcomp(norm_train[,predictors])
train = data.frame(pc_object$x)
test = data.frame(predict(pc_object, norm_test[,predictors]))
fracture = norm_train$fracture
train = cbind(train, fracture)
fracture = norm_test$fracture
test = cbind(test, fracture)
print(pc_object)
#Model
model_num = model_num + 1
model=qda(fracture~
            PC1+ PC1:PC2 +
            PC2+
            PC3+
            PC4,
          family="binomial",data=train)

#Roc and AUC:
class_preds=predict(model,newdata=test)$class
numeric_preds = ifelse(class_preds == "Yes",1,0)
auc = plot_roc_and_get_auc_generalized(numeric_preds, test, "fracture")

#Best Threshold using Balanced Accuracy
class_preds=predict(model,newdata=test)$class
CM_rep = confusionMatrix(table(class_preds, test$fracture), positive = pos_class)
acc = CM_rep$overall[1]
balanced_acc = CM_rep$byClass[11]
sens = CM_rep$byClass[1]
spec = CM_rep$byClass[2]
precision = CM_rep$byClass[5]
recall = CM_rep$byClass[6]
f1 = get_f1(precision, recall)

#Update Model Comparison Dataframe
model_stats = c(model_num, model_description, auc, acc, balanced_acc, sens, spec, f1)
model_compare_df = rbind(model_compare_df, model_stats)
```

## Random Forest

```{r}
model_description = "Random Forest"

#Var Selection
train = raw_train
test = raw_test

#Standardize Data
cts_vars = c("phy_id", "age", "weight", "height", "bmi", "fracscore")
train = get_standardized_df(train, cts_vars)
test = get_standardized_df(test, cts_vars)

#Specify Features and Target
features = c("phy_id", "priorfrac", "age", "weight", "bmi", "permeno", 
             "momfrac", "armassist", "smoke", "raterisk", "fracscore", 
             "bonemed", "bonemed_fu", "bonetreat", "height")
target = c("fracture")

#Find Optimal Depth
optimal_d = find_optimal_depth_f1(train, test, features, target, pos_class="Yes", d_max=20)

#Setup train and test data
train_fea = train %>% select(contains(features))
train_tar = train %>% select(contains(target))
test_fea = test %>% select(contains(features))
test_tar = test %>% select(contains(target))

#Build Final Model
model_num = model_num + 1
model = randomForest(x=train_fea, y=train_tar$fracture,
                      ntree = 1000, maxnodes = optimal_d)

#Roc and AUC: Have to make custom func for this
class_preds = unname(predict(model, test_fea))
numeric_preds = ifelse(class_preds == "Yes",1,0)
auc = plot_roc_and_get_auc_generalized(numeric_preds, test, "fracture")

#Best Threshold using Balanced Accuracy
pred_forest = predict(model, test_fea)
CM_rep = confusionMatrix(table(pred_forest, test$fracture), positive = pos_class)
CM_rep
acc = CM_rep$overall[1]
balanced_acc = CM_rep$byClass[11]
sens = CM_rep$byClass[1]
spec = CM_rep$byClass[2]
precision = CM_rep$byClass[5]
recall = CM_rep$byClass[6]
f1 = get_f1(precision, recall)

#Update Model Comparison Dataframe
model_stats = c(model_num, model_description, auc, acc, balanced_acc, sens, spec, f1)
model_compare_df = rbind(model_compare_df, model_stats)
```

## Final Comparison

```{r, echo = FALSE}
final_model_compare_df = model_compare_df[-1,-1]
final_model_compare_df$auc = as.numeric(final_model_compare_df$auc)
final_model_compare_df$accuracy = as.numeric(final_model_compare_df$accuracy)
final_model_compare_df$balanced_accuracy = as.numeric(final_model_compare_df$balanced_accuracy)
final_model_compare_df$sensitivity = as.numeric(final_model_compare_df$sensitivity)
final_model_compare_df$specificity = as.numeric(final_model_compare_df$specificity)
final_model_compare_df$f1_score = as.numeric(final_model_compare_df$f1_score)
final_model_compare_df = final_model_compare_df %>% mutate_if(is.numeric, round, digits = 3)
final_model_compare_df
```

Note that the `echo = FALSE` parameter was added to the code chunk to prevent printing of the R code that generated the plot.
