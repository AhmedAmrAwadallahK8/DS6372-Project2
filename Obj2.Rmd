---
title: "Objective 2"
author: "Ahmed Awadallah"
date: '2022-07-30'
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
#Import Libs
library(aplore3)
library(car)
library(glmnet)
library(epitools)
library(randomForest)
library(MASS)
library(mvtnorm)

#Change working directory to this source file directory
setwd(dirname(rstudioapi::getSourceEditorContext()$path))

#Load my custom functions
source("Utility.r")
```

```{r, include=FALSE}
#Data Pre Process
glow_bonemed_raw = glow_bonemed

#Don't need to do much cleaning we know this data is overall fine
#Remove features that should not be used in the model
#Choose variables
variablesToRemove = c("sub_id", "site_id")
glow_bonemed_raw = glow_bonemed_raw %>% select(-variablesToRemove)

#Split into training and test set
set.seed(1234)
train_test_list = get_train_test_list(glow_bonemed_raw , 0.8)
raw_train = train_test_list[[1]]
raw_test = train_test_list[[2]]

#Through our EDA we know the data is mostly clean we just need to handle the
#data imbalance

raw_train = get_df_with_upsampled_class(raw_train, "fracture", "Yes", 3)
```


```{r pressure, include=FALSE}
#Setup Comparison Dataframe
model_compare_df = data.frame(model_num = c(0), model_descrip = c("Null Model"), auc=c(0), accuracy=c(0), balanced_accuracy=c(0)
                              , sensi=c(0), speci=c(0), f1_score=c(0))
model_compare_df
model_num = 0

pos_class = "Yes"
```

## Complex Logistic Regression

Strategy: Expand upon the simple logistic model by adding new terms and interactions

Final Model: fracture~
  age +
  height + tan(harmonic_height) + tan(harmonic_height):bmi +
  bmi  + bmi:bonemed +
  momfrac+
  armassist+
  raterisk+
  bonemed+
  bonemed_fu +
  bonetreat

```{r, include=FALSE}
model_description = "Complex LogReg"

#Var Selection
variablesToSelect = c("phy_id", "priorfrac", "age", "height", "bmi", "premeno", "momfrac", 
                      "armassist", "raterisk", "bonemed", "bonemed_fu", 
                      "bonetreat", "fracture")
train = raw_train %>% select(variablesToSelect)
test = raw_test %>% select(variablesToSelect)

#Add New Cts Features
train$harmonic_age = 1/train$age
train$log_age =log(train$age)

train$harmonic_height = 1/train$height
train$log_height =log(train$height)

train$harmonic_bmi = 1/train$bmi
train$log_bmi =log(train$bmi)

test$harmonic_age = 1/test$age
test$log_age =log(test$age)

test$harmonic_height = 1/test$height
test$log_height =log(test$height)

test$harmonic_bmi = 1/test$bmi
test$log_bmi =log(test$bmi)

#Standardize Data
cts_vars = c("age", "harmonic_age", "log_age", "height", "harmonic_height", "log_height", "bmi", "harmonic_bmi", "log_bmi")
train = get_standardized_df(train, cts_vars)
test = get_standardized_df(test, cts_vars)

#Model
model_num = model_num + 1
model=glm(fracture~
            age +
            height + tan(harmonic_height) + tan(harmonic_height):bmi +
            bmi  + bmi:bonemed +
            momfrac+
            armassist+
            raterisk+
            bonemed+
            bonemed_fu +
            bonetreat
          ,family="binomial",data=train)

```
#### ROC Plot

The following is the ROC plot for the model. The AUC metric will be shown at the
end in the metric comparison DF.

```{r, echo=FALSE}
#Roc and AUC
auc = plot_roc_and_get_auc(model, test, "fracture")
```

```{r, include=FALSE}
#Best Threshold using Balanced Accuracy
preds = unname(predict(model, test, type="response"))
```

#### Threshold Plot

This is the plot used to select the threshold that maximize the f1 score for the
model

```{r, echo=FALSE}
best_threshold = get_and_plot_best_threshold(preds, test$fracture, pos_class)
```

```{r, include=FALSE}
class_preds = ifelse(preds > best_threshold,"Yes","No")
CM_rep = confusionMatrix(table(class_preds, test$fracture), positive = pos_class)
acc = CM_rep$overall[1]
balanced_acc = CM_rep$byClass[11]
sens = CM_rep$byClass[1]
spec = CM_rep$byClass[2]
precision = CM_rep$byClass[5]
recall = CM_rep$byClass[6]
f1 = get_f1(precision, recall)

#Update Model Comparison Dataframe
model_stats = c(model_num, model_description, auc, acc, balanced_acc, sens, spec, f1)
model_compare_df = rbind(model_compare_df, model_stats)
```

## PCA and QDA

Strategy: Build a QDA model with principle components for the same prediction task. 

```{r, echo=FALSE}
model_description = "QDA + PC"

#Var Selection
variablesToSelect = c("phy_id", "age", "height", "bmi", "fracscore", "fracture")
norm_train = raw_train %>% select(variablesToSelect)
norm_test = raw_test %>% select(variablesToSelect)

#Standardize Data
cts_vars = c("phy_id", "age", "height", "bmi", "fracscore")
norm_train = get_standardized_df(norm_train, cts_vars)
norm_test = get_standardized_df(norm_test, cts_vars)

#Setup PC Train Test Dataframes
predictors = c("phy_id", "age", "height", "bmi", "fracscore")
pc_object = prcomp(norm_train[,predictors])
train = data.frame(pc_object$x)
test = data.frame(predict(pc_object, norm_test[,predictors]))
fracture = norm_train$fracture
train = cbind(train, fracture)
fracture = norm_test$fracture
test = cbind(test, fracture)
#Model
model_num = model_num + 1
model=qda(fracture~
            PC1+ PC1:PC2 +
            PC2+
            PC3+
            PC4,
          family="binomial",data=train)
```

#### Principle Component Interpretation

PC1 appears to show something about age and fracscore

PC2 focuses more on physician, height and BMI

PC3 is physician and BMI

PC4 is physician, height, BMI, and fracscore

PC5 is age and fracscore again

Overall there is quite a bit of redundancy especially from PC4 and PC5. Hard to
say what these are exactly representing overall.

```{r, echo=FALSE}
print(pc_object)
```

#### ROC

The following is the ROC plot for the model. The AUC metric will be shown at the
end in the metric comparison DF.

```{r, echo=FALSE}
#Roc and AUC:
class_preds=predict(model,newdata=test)$class
numeric_preds = ifelse(class_preds == "Yes",1,0)
auc = plot_roc_and_get_auc_generalized(numeric_preds, test, "fracture")
```

```{r, include=FALSE}
#Best Threshold using Balanced Accuracy
class_preds=predict(model,newdata=test)$class
CM_rep = confusionMatrix(table(class_preds, test$fracture), positive = pos_class)
acc = CM_rep$overall[1]
balanced_acc = CM_rep$byClass[11]
sens = CM_rep$byClass[1]
spec = CM_rep$byClass[2]
precision = CM_rep$byClass[5]
recall = CM_rep$byClass[6]
f1 = get_f1(precision, recall)

#Update Model Comparison Dataframe
model_stats = c(model_num, model_description, auc, acc, balanced_acc, sens, spec, f1)
model_compare_df = rbind(model_compare_df, model_stats)
```

## Random Forest

Strategy: Use a random forest for the prediction task. Plan is to give the model
all the features present in the dataset as random forest has its own innate
feature selection. Perhaps it picks something out I couldn't see.

```{r, echo=FALSE}
model_description = "Random Forest"

#Var Selection
train = raw_train
test = raw_test

#Standardize Data
cts_vars = c("phy_id", "age", "weight", "height", "bmi", "fracscore")
train = get_standardized_df(train, cts_vars)
test = get_standardized_df(test, cts_vars)

#Specify Features and Target
features = c("phy_id", "priorfrac", "age", "weight", "bmi", "permeno", 
             "momfrac", "armassist", "smoke", "raterisk", "fracscore", 
             "bonemed", "bonemed_fu", "bonetreat", "height")
target = c("fracture")

```

#### Optimal Depth Plot

This is how the depth parameter is optimized. Forest classifiers are trained from
some minimum depth up to some maximum depth. The depth that gave the best F1 score
is then selected. The following is a plot visualizing the results.

```{r, echo=FALSE}
#Find Optimal Depth
optimal_d = find_optimal_depth_f1(train, test, features, target, pos_class="Yes", d_max=60)
```

```{r, include=FALSE}
#Setup train and test data
train_fea = train %>% select(contains(features))
train_tar = train %>% select(contains(target))
test_fea = test %>% select(contains(features))
test_tar = test %>% select(contains(target))

#Build Final Model
model_num = model_num + 1
model = randomForest(x=train_fea, y=train_tar$fracture,
                      ntree = 1000, maxnodes = optimal_d)
```

#### ROC

The following is the ROC plot for the model. The AUC metric will be shown at the
end in the metric comparison DF.

```{r, echo=FALSE}
#Roc and AUC: Have to make custom func for this
class_preds = unname(predict(model, test_fea))
numeric_preds = ifelse(class_preds == "Yes",1,0)
auc = plot_roc_and_get_auc_generalized(numeric_preds, test, "fracture")
```

```{r, include=FALSE}
#Best Threshold using Balanced Accuracy
pred_forest = predict(model, test_fea)
CM_rep = confusionMatrix(table(pred_forest, test$fracture), positive = pos_class)
CM_rep
acc = CM_rep$overall[1]
balanced_acc = CM_rep$byClass[11]
sens = CM_rep$byClass[1]
spec = CM_rep$byClass[2]
precision = CM_rep$byClass[5]
recall = CM_rep$byClass[6]
f1 = get_f1(precision, recall)

#Update Model Comparison Dataframe
model_stats = c(model_num, model_description, auc, acc, balanced_acc, sens, spec, f1)
model_compare_df = rbind(model_compare_df, model_stats)
```

## Final Comparison

As we can see the complex logistic regression model did the best, followed by PCA+QDA
then random forest.

```{r, echo = FALSE}
final_model_compare_df = model_compare_df[-1,-1]
final_model_compare_df$auc = as.numeric(final_model_compare_df$auc)
final_model_compare_df$accuracy = as.numeric(final_model_compare_df$accuracy)
final_model_compare_df$balanced_accuracy = as.numeric(final_model_compare_df$balanced_accuracy)
final_model_compare_df$sensi = as.numeric(final_model_compare_df$sensi)
final_model_compare_df$speci = as.numeric(final_model_compare_df$speci)
final_model_compare_df$f1_score = as.numeric(final_model_compare_df$f1_score)
final_model_compare_df = final_model_compare_df %>% mutate_if(is.numeric, round, digits = 3)
grid.table(final_model_compare_df)
```
